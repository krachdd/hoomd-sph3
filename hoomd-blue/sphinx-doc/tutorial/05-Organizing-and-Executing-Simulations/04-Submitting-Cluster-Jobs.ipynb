{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting Cluster Jobs\n",
    "\n",
    "## Overview\n",
    "\n",
    "### Questions\n",
    "\n",
    "* How do I submit workflows on HPC systems? \n",
    "* How can I combine many simulations into a single cluster job?\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Demonstrate the use of **resources** to execute with **MPI domain decomposition**.\n",
    "* Demonstrate the use of **groups** to execute an action on multiple jobs _in parallel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Progress bars do not format well in notebook output.\n",
    "os.environ[\"ROW_NO_PROGRESS\"] = \"true\"\n",
    "# We do not want to encourage users to use --yes, but the tutorials are not interactive.\n",
    "os.environ[\"ROW_YES\"] = \"true\"\n",
    "# Pretend to be on the greatlakes cluster\n",
    "os.environ[\"ROW_CLUSTER\"] = \"anvil\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Jobs\n",
    "\n",
    "On HPC systems, you submit **cluster jobs** to the queue which then execute on the compute nodes.\n",
    "**Row** generates **cluster job** submission scripts that execute the **actions** in your **workflow**.\n",
    "HPC systems provide access to many more CPU cores or GPUs than are available on the typical workstation.\n",
    "Use them to execute the most time-consuming **actions**.\n",
    "You can increase the **throughput** (number of simulations per unit time) by executing many simulations *in parallel*, reduce **latency** (time to complete a single simulation) by executing each simulation on more than one MPI rank with **domain decomposition**, or combine the two.\n",
    "\n",
    "This tutorial will demonstrate how to use **row** to submit HOOMD-blue **actions** in **cluster jobs**.\n",
    "See [Parallel Simulations with MPI](../03-Parallel-Simulations-With-MPI/00-index.ipynb) to learn more about MPI **domain decomposition**. See the [row user documentation](https://row.readthedocs.io) and your HPC center's documentation for an introduction to HPC job queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain decomposition\n",
    "\n",
    "The **resources** table `workflow.toml` instructs **row** to request the chosen resources in the **cluster job**. \n",
    "The values used in this tutorial are for example purposes only.\n",
    "You should choose a number of MPI ranks, equilibration steps, and cluster job walltime appropriate for your project.\n",
    "\n",
    "HOOMD-blue uses MPI for interprocess communication.\n",
    "In this case, the 2 *processes per directory* equate to 2 **MPI ranks** per directory using domain decomposition:\n",
    "\n",
    "```toml\n",
    "[action.resources]\n",
    "processes.per_directory = 2\n",
    "```\n",
    "\n",
    "You also need to select the ``mpi`` launcher:\n",
    "\n",
    "```toml\n",
    "[[action]]\n",
    "...\n",
    "launchers = [\"mpi\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing many simulations in parallel\n",
    "\n",
    "Some HPC systems schedule jobs *only by full node* or limit the *number of cluster jobs* you can queue at one time.\n",
    "You may also have many thousands of **directories** and want to reduce the time spent waiting in queue.\n",
    "In both of these cases, you can use MPI to **group** the execution of an **action** on many **directories** into one **cluster job** (see [Parallel Simulations with MPI](../03-Parallel-Simulations-With-MPI/00-index.ipynb) for an introduction to MPI **partitions**).\n",
    "You can use MPI partitions alone (one rank per **directory**) or in combination with MPI **domain decomposition** (more than one rank per **directory**).\n",
    "\n",
    "**Row** *groups* eligible **directories** together every time it **submits** an **action** for execution.\n",
    "It **groups** *ALL* eligible directories *by default*.\n",
    "The *randomize* and *compress* **actions** in the previous section take `*jobs` as an argument and loop over the directories *in serial*.\n",
    "When executing in serial like this, the **wall time** scales with the number of directories.\n",
    "Those actions correspondingly set `resources.walltime.per_directory` and **row** computes the total **wall time** needed for the **cluster job** based on the **group's** size.\n",
    "*Randomize* and *compress* execute very quickly, so the total wall times to process the directories *in serial* are manageable.\n",
    "\n",
    "The *equilibrate* **action** takes much longer, so it is not feasible to execute in serial.\n",
    "To execute *equilibrate* in parallel, you need to:\n",
    "1) Launch up to `maximum_size` directories per **cluster job**.\n",
    "   Set `walltime.per_submission` so that the total **wall time** does not scale with the number of directories.\n",
    "   ```toml\n",
    "   [action.group]\n",
    "   maximum_size = 64\n",
    "\n",
    "   [action.resources]\n",
    "   walltime.per_submission = \"12:00:00\"\n",
    "   ```\n",
    "\n",
    "3) Choose the job based on the partition in `equilibrate.py` (shown in the previous section):\n",
    "   ```python\n",
    "    communicator = hoomd.communicator.Communicator(\n",
    "        ranks_per_partition=RANKS_PER_PARTITION\n",
    "    )\n",
    "    job = jobs[communicator.partition]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting cluster jobs\n",
    "\n",
    "The example is small for demonstration purposes.\n",
    "It uses 2 MPI ranks per **directory** and executes all three **directories** in one **cluster job** (in parallel).\n",
    "In production work you should choose the number of ranks per **directory** (`processes.per_directory`) and the number of **directories** per **cluster job** (`maximum_size`) to utilize an integer number of whole nodes in each **cluster job** leaving no empty cores or GPUs.\n",
    "For example use 16 ranks per **directory** and 32 **directories** per **cluster job** to use 4 whole 128-core nodes per **cluster job**.\n",
    "When the total number of **directories** is not an integer multiple of `maximum_size`, **row** will form one smaller *tail* **group** to cover the remainder.\n",
    "If it doesn't fill a whole node, **Row** will submit the *tail* **cluster job** to a shared queue (if available on the HPC system).\n",
    "\n",
    "Here is the full `workflow.toml` that would allow **cluster jobs** to use up to one full 128-core node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"workflow.toml\", \"w\") as workflow:\n",
    "    workflow.write(\"\"\"\n",
    "[default.action]\n",
    "command = \"python project.py --action $ACTION_NAME {directories}\"\n",
    "submit_options.anvil.account = \"my_account\"\n",
    "\n",
    "[[action]]\n",
    "name = \"randomize\"\n",
    "products = [\"random.gsd\"]\n",
    "resources.walltime.per_directory = \"00:05:00\"\n",
    "\n",
    "[[action]]\n",
    "name = \"compress\"\n",
    "previous_actions = [\"randomize\"]\n",
    "products = [\"compressed.gsd\"]\n",
    "resources.walltime.per_directory = \"00:10:00\"\n",
    "\n",
    "[[action]]\n",
    "name = \"equilibrate\"\n",
    "previous_actions = [\"compress\"]\n",
    "products = [\"trajectory.gsd\"]\n",
    "launchers = [\"mpi\"]\n",
    "\n",
    "[action.group]\n",
    "maximum_size = 64\n",
    "\n",
    "[action.resources]\n",
    "processes.per_directory = 2\n",
    "walltime.per_submission = \"12:00:00\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes that you are using the [Purdue Anvil](https://www.rcac.purdue.edu/knowledge/anvil) cluster.\n",
    "It sets a placeholder cluster account in the default action `submit_options.anvil.account`.\n",
    "Adjust the account name and/or the cluster name as appropriate for your HPC system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mAction     \u001b[0m \u001b[4mCompleted\u001b[0m \u001b[4mSubmitted\u001b[0m \u001b[4mEligible\u001b[0m \u001b[4mWaiting\u001b[0m \u001b[4mRemaining cost\u001b[0m\n",
      "\u001b[1mrandomize  \u001b[0m \u001b[32m\u001b[1m        3\u001b[0m \u001b[33m\u001b[1m        0\u001b[0m \u001b[34m       0\u001b[0m \u001b[36m\u001b[2m      0\u001b[0m\n",
      "\u001b[1mcompress   \u001b[0m \u001b[32m\u001b[1m        3\u001b[0m \u001b[33m\u001b[1m        0\u001b[0m \u001b[34m       0\u001b[0m \u001b[36m\u001b[2m      0\u001b[0m\n",
      "\u001b[1mequilibrate\u001b[0m \u001b[32m\u001b[1m        0\u001b[0m \u001b[33m\u001b[1m        0\u001b[0m \u001b[34m       3\u001b[0m \u001b[36m\u001b[2m      0\u001b[0m \u001b[2m\u001b[3m  72 CPU-hours\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! row show status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *equilibrate* step is ready to execute.\n",
    "**Row** automates the **cluster job** submission process with `row submit`.\n",
    "Use the `--dry-run` flag first to ensure that the generated **cluster jobs** are correct (`--dry-run` displays the generated submission script(s) and skips the submission step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --job-name=equilibrate-59363805e6f46a715bc154b38dffc4e4+2\n",
      "#SBATCH --output=equilibrate-%j.out\n",
      "#SBATCH --partition=shared\n",
      "#SBATCH --ntasks=6\n",
      "#SBATCH --mem-per-cpu=1800M\n",
      "#SBATCH --time=720\n",
      "#SBATCH --account=my_account\n",
      "\n",
      "directories=(\n",
      "59363805e6f46a715bc154b38dffc4e4\n",
      "972b10bd6b308f65f0bc3a06db58cf9d\n",
      "c1a59a95a0e8b4526b28cf12aa0a689e\n",
      ")\n",
      "\n",
      "export ACTION_WORKSPACE_PATH=workspace\n",
      "export ACTION_CLUSTER=anvil\n",
      "export ACTION_NAME=equilibrate\n",
      "export ACTION_PROCESSES=6\n",
      "export ACTION_WALLTIME_IN_MINUTES=720\n",
      "export ACTION_PROCESSES_PER_DIRECTORY=2\n",
      "\n",
      "trap 'printf %s\\\\n \"${directories[@]}\" | /Users/joaander/.cargo/bin/row scan --no-progress -a equilibrate - || exit 3' EXIT\n",
      "srun --mpi=pmi2 --ntasks=6 python project.py --action $ACTION_NAME \"${directories[@]}\" || { >&2 echo \"[row] Error executing command.\"; exit 1; }\n"
     ]
    }
   ],
   "source": [
    "! row submit --dry-run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this configuration, **row** submits one **cluster job** that launches `project.py` with 6 ranks.\n",
    "\n",
    "The partitioned communicator\n",
    "```python\n",
    "hoomd.communicator.Communicator(ranks_per_partition=2)\n",
    "```\n",
    "will form 3 partitions, each using 2 MPI ranks to **domain decompose** the simulation.\n",
    "The partition index selects which directory each partition will execute:\n",
    "```python\n",
    "job = jobs[communicator.partition]\n",
    "```\n",
    "\n",
    "If there were more than 64 **directories** in this workspace, **row** would generate more than one **cluster job** due to the `maximum_size = 64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are sure that the resources are configured correctly, you can execute `row submit` on a cluster to submit the job to the queue.\n",
    "**Row** will track the **cluster jobs** and prevent you from submitting an action on a directory that is still in queue or running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section of the tutorial, you configured the *eqilibrate* action to execute on many directories in parallel and used **row** to generate a **cluster job** for submission.\n",
    "\n",
    "This is the end of the tutorial on organizing and executing simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial only teaches the basics of **row**.\n",
    "Read the [row documentation](https://row.readthedocs.io/) to learn more."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "record_timing": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
